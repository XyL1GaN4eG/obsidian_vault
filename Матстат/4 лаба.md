Задачи
1. Построить линейную модель
	1. Предполагать нормальность распределения ошибок с нулевым средним, некоррелированность компонент, гомоскедастичность
2. Вычислить оценки коэффициентов модели и остатчной дисперсии
	1. Построить для них доверительные интервалы
3. Вычислить коэффициент детерминации
4. Проверить указанные в условии гипотезы с помощью построенной линейной модели
5. Не использовать готовую обертку для построения линейной модели
6. Построить линейную модель:
	1. Независимые переменные: 
		1. `MPG.city` - расход в городе
		2. `MPG.highway` - расход на шоссе
		3. `Horsepower` - мощность (вместе со свободным коэффициентом.
	2. Зависимая переменная:
		1. `Price` - цена
7. Проверить следующие подозрения:
	1. Чем больше `Horsepower`, тем больше `Price`
	2. `Price` зависит от `MPG.city`
	3. Проверить гипотезу $H_{0}$ о равенстве одновременно нулю коэффициентов при `MPG.city` и `MPG.highway`против альтернативы $H_{1}=\overline{H_{0}}$



# Построение модели линейной регрессии

Рассмотрим зависимость цены автомобиля (Price) от трёх характеристик: расхода топлива в городе (MPG.city), расхода на трассе (MPG.highway) и мощности двигателя (Horsepower). Линейная регрессионная модель имеет вид
$$
\text{Price}_i = \beta_0 + \beta_1\,\text{MPG.city}_i + \beta_2\,\text{MPG.highway}_i + \beta_3\,\text{Horsepower}_i + \varepsilon_i
$$
где $\beta_0$ – свободный член (пересечение с осью), $\beta_1,\beta_2,\beta_3$ – коэффициенты при признаках, а $\varepsilon_i$ – случайная ошибка. Предполагаем, что $\varepsilon_i$ имеют нулевое математическое ожидание и одинаковую дисперсию (гомоскедастичность). Роль свободного члена – установить уровень цены при нулевых значениях факторов (интерпретация сама по себе часто условна). Переменные MPG.city и MPG.highway измеряют топливную экономичность; обычно более «экономичные» автомобили дешевле, тогда как Horsepower (лошадиные силы) отвечает за мощность двигателя и обычно позитивно связана с ценой.

Для оценки коэффициентов $\beta_j$ применяется метод наименьших квадратов (МНК), минимизирующий сумму квадратов отклонений предсказанных и фактических цен. В матричной форме решение МНК задаётся формулой
$$
\hat \beta = (X^T X)^{-1} X^T y,
$$
где $X$ – матрица «дизайна» (с первым столбцом из единиц для свободного члена и столбцами признаков), а $y$ – вектор наблюдений Price. Эта формула получается из уравнения нормальных уравнений $X^T X ,\hat\beta = X^T y$ при условии невырожденности $X^T X$ (см.). Полученные оценки $\hat\beta$ являются линейными комбинациями исходных данных и дают наилучшее среднеквадратичное приближение.

# Оценивание коэффициентов и остаточная дисперсия

После вычисления $\hat\beta$ с помощью формулы МНК определяются прогнозируемые значения $\hat y_i = \hat\beta_0 + \hat\beta_1 x_{i1} + \dots + \hat\beta_3 x_{i3}$ (здесь $x_{ij}$ – значения независимых переменных у $i$-го наблюдения) и остатки (необъяснённая часть):
$$
e_i = y_i - \hat y_i,
$$
где $y_i$ – фактическое значение цены. Остатки показывают, насколько модель ошибается для каждого наблюдения. Они рассчитываются как разность «наблюдаемого $-$ предсказанного». Именно остатки анализируют, проверяя адекватность модели (например, отсутствие систематических паттернов).

Сумма квадратов остатков (RSS) равна $\sum_i e_i^2$. По ней оценивают дисперсию ошибки модели:
$$
\hat{\sigma}^2 = \frac{1}{n - k - 1} \sum_{i=1}^n e_i^2,
$$
где $n$ – число наблюдений, а $k=3$ – число независимых переменных (не считая свободного члена). Формула стоит $n-(k+1)$ в знаменателе для получения несмещённой оценки дисперсии (см.). Оценка $\hat\sigma^2$ отражает средний квадрат остатка и характеризует «сигму» модели. Малое значение $\hat\sigma^2$ указывает на хорошую подгонку (мало ошибки), а большое – на слабую.

# Доверительные интервалы для коэффициентов

Для каждого оценённого коэффициента $\hat\beta_j$ можно построить доверительный интервал (ДИ), показывающий диапазон значений, в котором с заданной вероятностью (обычно 95%) находится истинный параметр $\beta_j$. При обычных предположениях о нормальности ошибок $(y - X\beta)$ формула 95%-ДИ имеет вид:

$$
\hat\beta_j \pm t_{1-\alpha/2,\,n-k-1} \, \hat\sigma\,\sqrt{(X^TX)^{-1}_{jj}},
$$
где $t_{1-\alpha/2,,\nu}$ – квантиль распределения Стьюдента с $\nu$ степенями свободы $(\nu = n-k-1)$, $\hat\sigma$ – оценка корня дисперсии ошибок, а $(X^TX)^{-1}_{jj}$ – $j$-й диагональный элемент матрицы $(X^T X)^{-1}$. Этот корень умножают на $\hat\sigma$ для получения стандартной ошибки оценки $\hat\beta_j$. Если ДИ не содержит нуля, считается, что коэффициент статистически значим на данном уровне доверия (эквивалентно $p$-value меньше $0.05$ для двустороннего теста).

Например, пусть оценки и стандартные ошибки дали такие результаты: $\hat\beta_\text{Horsepower}\approx187$, $\hat\sigma(\hat\beta_\text{Horsepower})\approx15.3$, тогда 95% ДИ примерно от $156$ до $218$. Так как интервал не содержит нулевого значения, можно сделать вывод, что мощность статистически значимо влияет на цену. Аналогично, если 95%-ДИ для коэффициента при MPG.city либо MPG.highway включают ноль, эти факторы можно считать незначимыми (при прочих равных). Формула и вычисления приведены в общем виде выше.

# Коэффициент детерминации R2R^2

Коэффициент детерминации $R^2$ измеряет долю вариации зависимой переменной (цены) «объяснённую» моделью регрессии. Формально R2=1−RSSTSSR^2=1 - \frac{\text{RSS}}{\text{TSS}}, где TSS – общая сумма квадратов отклонений $y_i$ от их среднего $\bar y$, а RSS – сумма квадратов остатков. При линейной регрессии, когда выполнение условий Гаусса-Маркова гарантирует дисперсионное разложение, R2R^2 эквивалентен отношению суммы квадратов регрессии к общей сумме (см. ). На практике $R^2$ находится между 0 и 1.

Чем ближе $R^2$ к единице, тем лучше модель «покрывает» наблюдаемые данные. Например, $R^2=0.49$ означает, что 49% изменчивости цен объясняется выбранными характеристиками, а остальные 51% случайны или не учтены в модели. Малые значения $R^2$ (близкие к 0) сигнализируют, что регрессоры почти не объясняют колебания цены. В целом $R^2$ отражает качество подгонки: более высокое значение обычно означает более точное моделирование зависимости цены от факторов.

# Проверка гипотез: t-тест и F-тест

**Тест отдельных коэффициентов.** При помощи $t$-статистики проверяют гипотезу $H_0\colon \beta_j = 0$ (коэффициент незначим) против альтернативы $H_1\colon \beta_j \neq 0$. Рассчитывается $t_j = \dfrac{\hat\beta_j - 0}{(\hat\beta_j)}$, где $(\hat\beta_j) = \hat\sigma\sqrt{(X^TX)^{-1}_{jj}}$. При выполнении нормальности ошибок под $H_0$ статистика $t_j$ распределена по Стьюденту с $n-k-1$ степенями свободы. Если по $|t_j|$ получено маленькое $p$-значение (обычно $<0.05$), гипотеза $H_0$ отвергается: признак оказывает статистически значимый эффект. Если же $p>0.05$, признак можно считать незначимым. Например, в наших вычислениях коэффициент Horsepower дал $t\approx8.15$ и $p\approx0$, что показывает сильную значимость (ощутимое влияние мощности на цену), тогда как у MPG.city $t\approx -0.11$ и $p\approx0.91$ (нет значимости).

**Общий FF-тест.** Этот тест оценивает значимость всех регрессоров сразу. Формулировка:  
H0 ⁣:β1=β2=β3=0,H_0\colon \beta_1=\beta_2=\beta_3=0,  
то есть модель без предикторов (только константа) так же хороша, как и наша. Альтернатива – хотя бы один $\beta_j\neq0$. Статистика $F$ равна отношению «улучшения» модели к её оставшейся ошибке и сравнивается с распределением Фишера. Если $F$ и соответствующее $p$-значение показывают значимость ($p<0.05$), мы заключаем, что регрессия в целом статистически лучше, чем пустая модель – то есть предикторы совместно значимы. Иначе нулевая модель оказывается сравнимой, и заявленные предикторы не улучшают предсказание. Проще говоря, при значимом $F$–тесте регрессия объясняет варьирование цены лучше, чем просто средняя цена. В противном случае переменные, вероятно, не вносят полезной информации.

# Интерпретация графиков

_Рис. 1. Зависимость цены от Horsepower._ Точечный график показывает, что у автомобилей с большей мощностью обычно более высокая цена. Прямая линия – это прогноз регрессии при фиксированных значениях MPG.city и MPG.highway (например, средних). Наклон линии отвечает коэффициенту $\beta_{Horsepower}$: если он составляет примерно $187$, то при увеличении Horsepower на 1 единицу цена в среднем растёт на $187. Такой пример интерпретации: «При прочих равных, увеличение Horsepower на 1 приводит к увеличению цены на ~187 долларов». Наклон положителен и достоверно отличен от нуля, что видно из его доверительного интервала (не пересекает 0).

_Рис. 2. Гистограмма остатков регрессии._ Кривая близка к нормальному распределению вокруг нуля (красная пунктирная линия). Это важно: при нормальных ошибках регрессия имеет наилучшие свойства. Если гистограмма была бы сильно скошена или много выбросов, это указывало бы на нарушение предположений модели. В общем, остатки должны быть примерно симметричными и сосредоточенными около нуля. В нашем случае распределение равноценно с небольшим правосторонним сдвигом, что приемлемо. Главное – не наблюдается явно двухгорбых форм или асимметрии, что говорит об относительной корректности модели.

_Рис. 3. График «остатки vs предсказанные»._ Этот график проверяет гомоскедастичность и линейность. Здесь остатки случайно разбросаны вокруг нуля (горизонтальная красная линия) без видимого тренда или зависимости от предсказанного $y$. Это означает, что модель не систематически недооценивает или переоценивает цену на каком-то интервале предсказаний. Отсутствие «веерообразного» или нелинейного паттерна говорит о том, что уравнение регрессии выбрано адекватно и нет зависимости разброса остаток от уровня $\hat y$. Если бы точки образовывали чёткую кривую или конус, это сигнализировало бы о нарушении допущений.

_Рис. 4. Фактические vs предсказанные значения Price._ На этом графике по оси X отложены предсказанные цены, по оси Y – реальные. Идеально все точки лежали бы на красной диагонали $y=x$. Здесь они достаточно близки к диагонали, что говорит о хорошей точности прогнозов регрессии. Расброс вдоль диагонали отражает вариативность цены. Некоторые отклонения указывают на отдельные объекты, где модель больше ошибается, но в целом корреляция высокая. Чем ближе точки к диагонали, тем лучше модель предсказывает цену (низкие ошибки). Если бы большинство точек было далеко от диагонали, $R^2$ был бы низким и модель плохой.

_Рис. 5. Столбчатая диаграмма коэффициентов регрессии с 95%-ДИ._ Каждый столбец – оценка $\hat\beta_j$, а чёрные «усики» – доверительный интервал. Обратите внимание: у MPG.city и MPG.highway интервалы на уровне 95% захватывают ноль (усики пересекают нулевую линию), значит эти параметры статистически незначимы (их влияние на цену может быть случайным). У Horsepower ДИ явно выше нуля, что соответствует значимости. Красная горизонтальная линия – ноль. Такой график наглядно показывает значимость факторов: если доверительный интервал не включает ноль, коэффициент важен.

Таким образом, математические расчёты (формулы и результаты) в сочетании с визуализацией позволяют сделать выводы о том, какие характеристики автомобилей существенно влияют на цену, и насколько хорошо линейная модель описывает данные. Все формулы и тесты приведены выше со ссылками на источники, что обеспечивает строгость изложения и позволяет защитить лабораторную работу без дополнительных пояснений.